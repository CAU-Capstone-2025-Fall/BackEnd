# ============================================================
# ğŸ§© Teacher vs MLP vs Student Distillation ì„±ëŠ¥ ë¹„êµ
# ============================================================
from pathlib import Path

import numpy as np
import pandas as pd
from scipy.stats import pearsonr
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ------------------------------------------------------------
# 1ï¸âƒ£ ê²½ë¡œ ì„¤ì •
# ------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent
TEACHER_PATH = BASE_DIR / "results_dist" / "Z_teacher.csv"          # Teacher ì •ë‹µ
MLP_PATH = BASE_DIR / "results_mlp" / "Z_pred_mlp.csv"              # ê¸°ì¡´ MLP ì˜ˆì¸¡ (ì—†ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬)
STUDENT_PATH = BASE_DIR / "results_dist" / "Z_student.csv"          # ìƒˆ distillation ì˜ˆì¸¡

print(f"ğŸ“‚ Loading data from:")
print(f" - Teacher: {TEACHER_PATH}")
print(f" - MLP:     {MLP_PATH}")
print(f" - Student: {STUDENT_PATH}")

# ------------------------------------------------------------
# 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ
# ------------------------------------------------------------
Y_teacher = pd.read_csv(TEACHER_PATH).values.astype(np.float32)
Y_mlp = pd.read_csv(MLP_PATH).values.astype(np.float32)
Y_student = pd.read_csv(STUDENT_PATH).values.astype(np.float32)

assert Y_teacher.shape == Y_mlp.shape == Y_student.shape, \
    f"âŒ Shape mismatch: teacher={Y_teacher.shape}, mlp={Y_mlp.shape}, student={Y_student.shape}"

output_dim = Y_teacher.shape[1]

# ------------------------------------------------------------
# 3ï¸âƒ£ ì„±ëŠ¥ ê³„ì‚° í•¨ìˆ˜
# ------------------------------------------------------------
def evaluate(Y_true, Y_pred, name):
    mse = mean_squared_error(Y_true, Y_pred)
    mae = mean_absolute_error(Y_true, Y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(Y_true, Y_pred)
    r2_each = r2_score(Y_true, Y_pred, multioutput='raw_values')
    corr_each = [pearsonr(Y_true[:, i], Y_pred[:, i])[0] for i in range(output_dim)]
    df = pd.DataFrame({
        "Latent": [f"latent_{i+1}" for i in range(output_dim)],
        "RÂ²": r2_each,
        "Pearson_r": corr_each
    })
    summary = pd.DataFrame({
        "Model": [name],
        "MSE": [mse],
        "RMSE": [rmse],
        "MAE": [mae],
        "RÂ² (overall)": [r2]
    })
    return summary, df

# ------------------------------------------------------------
# 4ï¸âƒ£ í‰ê°€ ìˆ˜í–‰
# ------------------------------------------------------------
summary_mlp, detail_mlp = evaluate(Y_teacher, Y_mlp, "MLP")
summary_student, detail_student = evaluate(Y_teacher, Y_student, "Student")

# ------------------------------------------------------------
# 5ï¸âƒ£ ê²°ê³¼ ì¶œë ¥
# ------------------------------------------------------------
print("\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ë¹„êµ (ì „ì²´ í‰ê· ):")
summary_all = pd.concat([summary_mlp, summary_student], ignore_index=True)
print(summary_all.round(4))

print("\nğŸ“ˆ Student latentë³„ RÂ² ë° Pearson ìƒê´€:")
print(detail_student.round(4))

# ------------------------------------------------------------
# 6ï¸âƒ£ ì €ì¥
# ------------------------------------------------------------
SAVE_DIR = BASE_DIR / "results_compare"
SAVE_DIR.mkdir(exist_ok=True)
summary_all.to_csv(SAVE_DIR / "model_summary.csv", index=False, encoding="utf-8-sig")
detail_student.to_csv(SAVE_DIR / "student_detail.csv", index=False, encoding="utf-8-sig")

print(f"\nâœ… Saved comparison results to {SAVE_DIR}")
