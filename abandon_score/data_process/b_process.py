# ============================================================
# ğŸ¶ B_processed_final_filtered.py
#   - survey.xlsx ë¡œë“œ
#   - ë³µìˆ˜ì‘ë‹µí˜•(0/1), Likert(0~1) ì •ê·œí™”
#   - ë°˜ë ¤ë™ë¬¼ ì‚¬ìœ¡ ê²½í—˜ ìˆìŒ(A1=1,2) + ìœ ê¸°ì¶©ë™ê²½í—˜(B3>1) í•„í„°ë§
# ============================================================

import re

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ============================================================
# 1ï¸âƒ£ íŒŒì¼ ë¡œë“œ
# ============================================================
file_path = "data/survey.xlsx"
df_raw = pd.read_excel(file_path, sheet_name="ë§ˆì´í¬ë¡œë°ì´í„°", header=[0, 1])
df_raw.columns = df_raw.columns.get_level_values(1)
df = df_raw.copy()

# ============================================================
# 2ï¸âƒ£ ê¸°ë³¸ ì •ë¦¬: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
# ============================================================
drop_cols = [
    "ID", "SQ1", "SQ2", "SQ2_R", "SQ3_2", "SQ3_2_R","A1_1_1","A1_1_2","A1_1_3","A4",
    "DQ1", "DQ2", "DQ2_ETC4", "DQ3", "DQ4", "DQ5"
]
drop_keywords = ["ê¸°íƒ€", "ETC", "OPEN", "ì¼ë ¨ë²ˆí˜¸", "ìì¹˜êµ¬", "ê¶Œì—­"]

df = df.drop(columns=[c for c in df.columns if any(k in str(c) for k in drop_keywords) or c in drop_cols],
             errors="ignore")

# ============================================================
# 3ï¸âƒ£ í•„í„°: ë°˜ë ¤ë™ë¬¼ ê²½í—˜ + ìœ ê¸°ì¶©ë™ ì²´í¬
# ============================================================
# A1 (ë°˜ë ¤ë™ë¬¼ ì‚¬ìœ¡ê²½í—˜): 1=í˜„ì¬, 2=ê³¼ê±°
df["A1"] = pd.to_numeric(df["A1"], errors="coerce")
mask_pet_experience = df["A1"].isin([1, 2])

# B3 (ìœ ê¸°ì¶©ë™ ê²½í—˜): 1~3 Likert, 1=ì „í˜€ì—†ë‹¤ â†’ 2~3 ì„ íƒì ìœ ì§€
df["B3"] = pd.to_numeric(df["B3"], errors="coerce")
mask_abandon_experience = df["B3"] >= 1

# ë‘ ì¡°ê±´ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ì‚¬ëŒë§Œ ë‚¨ê¹€
df_filtered = df[mask_pet_experience & mask_abandon_experience].copy()
print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(df_filtered)}ëª… (ì „ì²´ ì¤‘ {len(df_filtered)/len(df):.1%}) ìœ ì§€")

# ============================================================
# 4ï¸âƒ£ ë³µìˆ˜ì‘ë‹µí˜• ë§¤í•‘
# ============================================================
multi_mapping = {
    "A2#": {
        1: "ê´€ë¦¬ë¹„ìš©ë¶€ë‹´", 2: "ì´ì›ƒê°€ì¡±ê°ˆë“±", 3: "ì´ìƒí–‰ë™ìœ„ìƒë¬¸ì œ",
        4: "ì‹œê°„ê³µê°„ë¶€ì¡±", 5: "ì—¬í–‰ì™¸ì¶œê³¤ë€", 6: "ì£½ìŒìŠ¬í””", 7: "ê°€ì¶œì‹¤ì¢…",
    },
    "A3#": {
        1: "ê¹¨ë—í•˜ê²Œí‚¤ìš¸ìì‹ ì—†ìŒ", 2: "ì£¼ê±°í™˜ê²½ë‚˜ì¨", 3: "ì‚¬ìœ¡ë¹„ìš©ë¶€ë‹´",
        4: "ì‹œê°„ë¶€ì¡±", 5: "ê³µê°„ì—†ìŒ", 6: "ë™ë¬¼ì‹«ì–´í•¨", 7: "ê°€ì¡±ë°˜ëŒ€",
    },
    "A5#": {
        1: "ëƒ„ìƒˆì‹¬í•¨", 2: "í„¸ë‚ ë¦¼", 3: "ì†ŒìŒ", 4: "ëŒ€ì†Œë³€ì˜¤ì—¼",
        5: "ë¬¼ë¦¬ê±°ë‚˜ìœ„í˜‘", 6: "êµí†µì‚¬ê³ ", 7: "ê³µì›ì‹ë‹¹ë¶ˆí¸", 9: "í”¼í•´ì—†ìŒ",
    },
    "B1#": {
        1: "êµìœ¡ê´€ë¦¬", 2: "ì˜ˆë°©ì¹˜ë£Œ", 3: "ëª©ìš•ìš´ë™",
        4: "ì¢‹ì€ë¨¹ì´", 5: "ìŠµì„±êµìœ¡", 6: "ê³µì¤‘ê·œë²”",
    },
    "B2#": {
        1: "ë¹„ìš©ë¶€ë‹´", 2: "ê°€ì¡±ê°ˆë“±", 3: "ìœ„ìƒë¬¸ì œ", 4: "ì—¬ê±´ê³¤ë€", 5: "ì—¬í–‰ì–´ë ¤ì›€",
    },
    "A1_2": {
        1: "ì•„ì´ë“¤ì •ì„œêµìœ¡", 2: "ì˜ˆì˜ê³ ê·€ì—¬ì›Œì„œ", 3: "ì™¸ë¡œì›Œì„œ", 4: "ìš°ì—°íˆê¸°íšŒ", 5: "ìœ ê¸°ê²¬ë¶ˆìŒ",
    },
}

def clean_to_codes(value):
    """'1, 3', '1 3', '1;3' ë“±ì„ [1,3]ìœ¼ë¡œ ë³€í™˜"""
    if pd.isna(value):
        return []
    if isinstance(value, (int, float)):
        return [int(value)]
    tokens = re.split(r"[ ,;/]+", str(value).strip())
    return [int(t) for t in tokens if t.isdigit()]

df_result = df_filtered.copy()

for prefix, mapping in multi_mapping.items():
    target_cols = [c for c in df_result.columns if c.startswith(prefix.replace("#", ""))]
    if not target_cols:
        continue

    for col in target_cols:
        df_result[col] = df_result[col].apply(clean_to_codes)
        for code, label in mapping.items():
            new_col = f"{prefix.replace('#', '')}_{label}"
            has_code = df_result[col].apply(lambda lst: code in lst)
            if new_col not in df_result.columns:
                df_result[new_col] = has_code.astype(int)
            else:
                df_result[new_col] = df_result[new_col] | has_code.astype(int)

    df_result.drop(columns=target_cols, inplace=True, errors="ignore")

print("âœ… ë³µìˆ˜ì‘ë‹µí˜• ë³€í™˜ ì™„ë£Œ (ëª¨ë“  í•­ëª© 0/1 ì²˜ë¦¬ë¨)")

# ============================================================
# 5ï¸âƒ£ A1 ì›í•« ì¸ì½”ë”©
# ============================================================
if "A1" in df_result.columns:
    A1_map = {
        1: "í˜„ì¬_ë°˜ë ¤ë™ë¬¼_ìˆìŒ",
        2: "ê³¼ê±°ì—ëŠ”_ìˆì—ˆìœ¼ë‚˜_ì§€ê¸ˆì€_ì—†ìŒ",
        3: "ë°˜ë ¤ë™ë¬¼_ê²½í—˜_ì—†ìŒ",
    }
    for code, label in A1_map.items():
        df_result[f"A1_{label}"] = (df_result["A1"] == code).astype(int)
    df_result.drop(columns=["A1"], inplace=True)
print("âœ… A1 ì›í•« ì¸ì½”ë”© ì™„ë£Œ")

# ============================================================
# 6ï¸âƒ£ A4 Likert ì •ê·œí™”
# ============================================================
if "A4" in df_result.columns:
    df_result["A4"] = pd.to_numeric(df_result["A4"], errors="coerce").fillna(0)
    scaler_A4 = MinMaxScaler()
    df_result["A4_norm"] = scaler_A4.fit_transform(df_result[["A4"]])
    df_result.drop(columns=["A4"], inplace=True)
print("âœ… A4 ì •ê·œí™” ì™„ë£Œ (0~1)")

# ============================================================
# 7ï¸âƒ£ C5/C6 ìˆœìœ„í˜• â†’ ì›í•«
# ============================================================
C5_labels = {
    1: "ê¸°ë³¸ì†Œì–‘êµìœ¡", 2: "êµ¬ì¡°ë³´í˜¸", 3: "ì˜ˆë°©ë°ì¹˜ë£Œ", 4: "í›ˆë ¨ìŠµì„±í™”",
    5: "ì‚¬ë£Œìš©í’ˆêµ¬ì…", 6: "ì—¬í–‰ê´€ë¦¬", 7: "ì†Œë¹„ìí”¼í•´ìƒë‹´", 8: "ì¥ë¡€ì‹œì„¤", 10: "í•„ìš”ì‚¬ì—…ì—†ìŒ"
}

for col in ["C5_1", "C5_2", "C5_3"]:
    if col not in df_result.columns:
        continue
    df_result[col] = pd.to_numeric(df_result[col], errors="coerce")
    for code, label in C5_labels.items():
        if code == 9:  # ê¸°íƒ€ëŠ” ë¬´ì‹œ
            continue
        new_col = f"{col}_{label}"
        df_result[new_col] = (df_result[col] == code).astype(int)
    df_result.drop(columns=[col], inplace=True)

print("âœ… C5/C6 ìˆœìœ„í˜• ì›í•« ì¸ì½”ë”© ì™„ë£Œ")

# ============================================================
# 8ï¸âƒ£ Likert 0~1 ì •ê·œí™” ë¬¸í•­
# ============================================================
likert_cols = [
    "C1","C2","C3_1","C3_2","C3_3","C3_4","C3_5","C3_6","C3_7","C3_8","C4","C6",
]
likert_names = [
    "ë™ë¬¼ë³´í˜¸ì„¼í„° ìš´ì˜ ì¸ì§€ì •ë„","ì„œìš¸ì‹œì˜ í­ë„“ì€ ë™ë¬¼ë³´í˜¸ì„¼í„° ìš´ì˜ ì°¬ì„±ì •ë„",
    "ì‹œë¯¼ë³µì§€ ê´€ì  ì •ë¶€ ê´€ì‹¬ í•„ìš”","ìì¹˜êµ¬ ê¸°ëŠ¥ë§Œìœ¼ë¡œ ì •ë¶€ ì—­í•  ë¶€ì¡±","ì¤‘ì•™ì •ë¶€ ì„œìš¸ì‹œ ì»¨íŠ¸ë¡¤íƒ€ì›Œ ì—­í•  í•„ìš”",
    "ê³µê³µ ì‚¬ìœ¡ê´€ë¦¬êµìœ¡ ê°ˆë“±ì¡°ì • í•„ìš”","ë¯¼ê°„ë¶€ë¬¸ì´ ë‹´ë‹¹í•˜ê¸° ì–´ë ¤ìš´ ì˜ì—­ ì¡´ì¬","ë³µì§€ì‹œì„¤íˆ¬ìë³´ë‹¤ ì‹œë¯¼ë³µì§€ìš°ì„ ",
    "ë°˜ë ¤ì¸ ì±…ì„ ê°•ì¡° ê³µê³µì—­í•  ìµœì†Œ","ê³µê³µì‚¬ì—…ì‹œ ë¯¼ê°„ë‹¨ì²´ì‹œì„¤ í™œìš©","ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì •ë¶€ ì¤‘ìš” ì—­í•  ì¸ì‹","ë™ë¬¼ë³µì§€ì§€ì›ì‹œì„¤ì´ ë“¤ì–´ì„ ë‹¤ë©´ ì´ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹­ë‹ˆê¹Œ?"
]
rename_dict = dict(zip(likert_cols, likert_names))
existing_cols = [c for c in likert_cols if c in df_result.columns]
df_result.rename(columns=rename_dict, inplace=True)
likert_cols = [rename_dict[c] for c in existing_cols]

df_result[likert_cols] = df_result[likert_cols].apply(pd.to_numeric, errors="coerce").fillna(0)
scaler = MinMaxScaler()
df_result[likert_cols] = scaler.fit_transform(df_result[likert_cols])

print(f"âœ… Likert ì •ê·œí™” ì™„ë£Œ ({len(likert_cols)}ê°œ)")

# ============================================================
# 9ï¸âƒ£ ì €ì¥
# ============================================================
df_result = df_result.fillna(0)
output_path = "data/B_processed_filtered.csv"
df_result.to_csv(output_path, index=False, encoding="utf-8-sig")

print(f"ğŸ¯ ì €ì¥ ì™„ë£Œ: {output_path}")
print("ğŸ“˜ ì˜ˆì‹œ ì»¬ëŸ¼:", list(df_result.columns[:20]))
print(df_result[[c for c in df_result.columns if c.startswith("A2") or c.startswith("A3")]].head(10))
