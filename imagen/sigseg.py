import numpy as np
import torch
from PIL import Image
from transformers import CLIPSegForImageSegmentation, CLIPSegProcessor

# ëª¨ë¸ ë¡œë“œ
processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open("data/1.jpg").convert("RGB")

# í”„ë¡¬í”„íŠ¸ (ì¢‹ìŒ ğŸ‘)
prompt = ["a photo of an animal"]

# padding, truncation ì¶”ê°€ ğŸ‘‡
inputs = processor(
    text=prompt,
    images=image,
    return_tensors="pt",
    padding=True,
    truncation=True,
)

with torch.no_grad():
    outputs = model(**inputs)

# ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í¬ í‰ê· 
mask_logits = outputs.logits.mean(0).squeeze()
mask = torch.sigmoid(mask_logits).numpy()

# normalize + ê°ë§ˆ ë³´ì • (ëŒ€ë¶€ë¶„ ë‹¤ ê²€ì€ ë¬¸ì œ í•´ê²°)
mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-6)
mask = np.power(mask, 0.7)

# ì›ë³¸ í¬ê¸°ë¡œ resize í›„ ì €ì¥
mask_img = Image.fromarray((mask * 255).astype("uint8")).resize(image.size)
mask_img.save("data/1_clipseg_mask.png")
print("ğŸ¯ Mask saved to data/1_clipseg_mask.png")
